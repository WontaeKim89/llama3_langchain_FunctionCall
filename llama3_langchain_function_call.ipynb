{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6accd53e-fb05-42b7-a961-715cc27acb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee50fb8e-40fb-4a96-8019-11012cf70127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_cpp_python          0.2.76\n"
     ]
    }
   ],
   "source": [
    "!pip3 list |grep llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0272c5ac-17e2-4971-b868-a67a23539e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "@tool\n",
    "def korea_city_weather_info(city_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Tool Description : The weather information of the city inputted by the user is outputted.\n",
    "    Args : \n",
    "        \"city_name\" : Name of the city to look up the weather\n",
    "    \"\"\"\n",
    "    return \"오늘 서울의 온도는 최고 24.2도, 최저 13.5도를 보이며, 낮 한때 소나기가 있겠습니다.\"\n",
    "\n",
    "@tool\n",
    "def search_phonenumber(query: str) -> str:\n",
    "    \"\"\"장소에 대한 전화번호 검색 결과를 반환할 때 사용되는 도구입니다. 전화번호를 알고싶을때 사용하는 도구입니다.\"\"\"\n",
    "    return \"판교 몽중헌 전화번호: 010-1234-5678\\n\\n서울 OOO 전화번호: 02-123-4567\"\n",
    "\n",
    "@tool\n",
    "def get_date():\n",
    "    \"\"\"오늘의 날짜를 알고싶을때, 사용하는 도구입니다.\"\"\"\n",
    "    from datetime import datetime\n",
    "    date = datetime.now()\n",
    "    return f\"오늘은 {date.year}년{date.month}월{date.day}일 입니다.\"\n",
    "\n",
    "@tool\n",
    "def get_team_member(team_name:str):\n",
    "    \"\"\"\n",
    "    팀의 구성원 정보를 조회하는 도구입니다.\n",
    "    <Argument>\n",
    "    - team_name : 구성원을 조회할 팀명을 입력합니다.\n",
    "    <질문예시> \n",
    "        - 데이터시각화팀 인원보여줘\n",
    "        - 설계팀 구성원 보여줘\n",
    "        - 기획4팀 팀원이 누가있지?\n",
    "    \"\"\"\n",
    "    return '손철준, 김민지D, 이재엽, 김원태, 이흥배, 허재우, 오형록, 신유나, 이승연C, 김상웅, 이창윤, 양성렬 총 12명입니다. '\n",
    "\n",
    "\n",
    "@tool\n",
    "def animal_habitat_search(animal_name:str):\n",
    "    \"\"\"\n",
    "    동물의 주요 서식지 정보를 알고싶을때 사용하는 도구입니다.\n",
    "    \"\"\"\n",
    "    return f'{animal_name}의 주요서식지 : 아마존 강남구 31번지'\n",
    "\n",
    "\n",
    "@tool\n",
    "def working_time_search(employee_nm:str):\n",
    "    \"\"\"\n",
    "    조회한 직원의 근무시간을 알고싶을때 사용하는 도구입니다.\n",
    "    <Argument>\n",
    "    - employee_nm : 근무시간을 조회할 직원의 이름을 입력합니다.\n",
    "    \"\"\"\n",
    "    return f'{employee_nm}의 {datetime.now().year}년 {datetime.now().month}월 총 근무시간 : 999시간'\n",
    "\n",
    "tools = [korea_city_weather_info, search_phonenumber, multiply, get_date, get_team_member, animal_habitat_search, working_time_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a07a90-4a13-41ad-979e-dafea1e99b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /workspace/wontae_kim/llama-3-Korean-Bllossom-70B-gguf-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hslim\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 145088\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,145088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,145088]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,145088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,296982]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 144782\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 144783\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_K:  441 tensors\n",
      "llama_model_loader: - type q5_K:   40 tensors\n",
      "llama_model_loader: - type q6_K:   81 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens definition check successful ( 306/145088 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 145088\n",
      "llm_load_print_meta: n_merges         = 296982\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 70.83 B\n",
      "llm_load_print_meta: model size       = 39.77 GiB (4.82 BPW) \n",
      "llm_load_print_meta: general.name     = hslim\n",
      "llm_load_print_meta: BOS token        = 144782 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 144783 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 144791 '<|eot_id|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 8 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    3.31 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   637.59 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5188.75 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  5237.12 MiB\n",
      "llm_load_tensors:      CUDA2 buffer size =  4835.88 MiB\n",
      "llm_load_tensors:      CUDA3 buffer size =  4777.06 MiB\n",
      "llm_load_tensors:      CUDA4 buffer size =  4777.06 MiB\n",
      "llm_load_tensors:      CUDA5 buffer size =  4835.88 MiB\n",
      "llm_load_tensors:      CUDA6 buffer size =  5354.75 MiB\n",
      "llm_load_tensors:      CUDA7 buffer size =  5080.86 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =    44.00 MiB\n",
      "llama_kv_cache_init:      CUDA2 KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:      CUDA3 KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:      CUDA4 KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:      CUDA5 KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:      CUDA6 KV buffer size =    44.00 MiB\n",
      "llama_kv_cache_init:      CUDA7 KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  320.00 MiB, K (f16):  160.00 MiB, V (f16):  160.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.55 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA2 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA3 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA4 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA5 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA6 compute buffer size =   264.01 MiB\n",
      "llama_new_context_with_model:      CUDA7 compute buffer size =   371.39 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 9\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '144783', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': 'hslim', 'llama.vocab_size': '145088', 'general.file_type': '15', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '144782', 'llama.attention.head_count': '64', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n",
    "# !huggingface-cli download Bllossom/llama-3-Korean-Bllossom-70B-gguf-Q4_K_M --local-dir='YOUR-LOCAL-FOLDER-PATH'\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'Bllossom/llama-3-Korean-Bllossom-70B-gguf-Q4_K_M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Llama(\n",
    "    model_path='/workspace/wontae_kim/llama-3-Korean-Bllossom-70B-gguf-Q4_K_M.gguf',\n",
    "    n_ctx=1024,\n",
    "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946604b1-5e22-495c-a50f-f2d2a0614eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_arg_generator(user_input:str, tools):\n",
    "    print(f'\\n--> User Question : {user_input}\\n\\n')\n",
    "    tools_info = render_text_description_and_args(tools).replace('{', '{{').replace('}', '}}')\n",
    "    tools_name = [t.name for t in tools]\n",
    "    \n",
    "    query = f\"{user_input} --> 라는 질문과 관련된 tool_name과 Argument에 대해서 system_prompt를 참고한 양식으로 출력해줘\"\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    당신은 주어진 질문에 적절한 도구(Tool)을 선택하여 제시하는 역할을 합니다..\n",
    "    You are responsible for selecting and presenting the appropriate tool for the given question.\n",
    "    \n",
    "    * 사용자 질문(Question)에 대해 아래 'instructions'와 '출력예시'을 참고해서 사용할 Tool과 Tool에 input할 argument를 출력해라.\n",
    "    \n",
    "    <Instructions>\n",
    "    - 아래 Tool List의 도구 중, 사용자의 질문과 관련된 도구를 선택하고 도구 이름과 도구에 입력해야할 인수(args)를 아래와 같은 Json형태의 데이터로 출력해라\n",
    "    - 반드시 아래의 Example of output의 양식을 지켜 반드시 Json 형태로 출력하라\n",
    "    - Tool name은 반드시 'Tools Name'에 포함된 이름이어야한다.\n",
    "    - 'action'과 'action_input'에 포함될 key에는 공백이 절대로 들어갈 수 없다.\n",
    "    \n",
    "    <Tool List>\n",
    "    {tools_info}\n",
    "    \n",
    "    <Tools Name>\n",
    "    {tools_name}\n",
    "    \n",
    "    <출력예시>\n",
    "    {{\n",
    "        \"action\": '사용할 도구의 이름'\n",
    "        \"action_input\": '도구에 input되어야 할 argument를 전달합니다.'\n",
    "    }}\n",
    "\n",
    "    <출력예시 관련 지시사항>\n",
    "    - 'action'에 포함되는 도구이름에는 반드시 공백을 제거한다.\n",
    "    - 'action_input'에 포함되는 dictionary에는 type정보는 표시하지 않는다.\n",
    "    - 'action_input'에 포함되는 dictionary의 key는 반드시 공백을 제거한다.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "        ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize = False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        \"max_tokens\":1024,\n",
    "        \"stop\":[\"<|eot_id|>\"],\n",
    "        \"echo\":True, # Echo the prompt in the output\n",
    "        \"top_p\":0.9,\n",
    "        \"temperature\":0.001,\n",
    "    }\n",
    "    \n",
    "    resonse_msg = model(prompt, **generation_kwargs)\n",
    "    get_tool_info = resonse_msg['choices'][0]['text'][len(prompt):]\n",
    "    print(f'************* 모델을 통해 질문에 적합한 Tool 선택 및 Argument생성을 완료하였습니다.*************')\n",
    "    print(f'{get_tool_info}\\n ********************************************')\n",
    "    print(get_tool_info)\n",
    "    get_tool_info = get_tool_info.lower()\n",
    "    return get_tool_info\n",
    "\n",
    "\n",
    "def tool_executor_parser(tool_description):\n",
    "    stop_key = ['Type', 'string']\n",
    "    des =f\"\"\"\n",
    "    Action:```\n",
    "    {tool_description}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
    "    parser = ReActJsonSingleInputOutputParser()\n",
    "    action = parser.parse(des)\n",
    "    action.tool = action.tool.strip().replace('-', '_').replace(' ', '_').lower() # tool 이름 정리하자.\n",
    "    action.tool_input = {k.strip().replace('-', '_').replace(' ', '_').lower():v for k, v in action.tool_input.items() if k not in stop_key} # tool 내부의 인수를 정리하자.\n",
    "    print(f'\\n\\n1) Tool_Name : {action.tool} \\n2) Tool Input Argument : {action.tool_input}\\n********************************************')\n",
    "    return action\n",
    "\n",
    "\n",
    "def cutom_Tool_Executor(user_input, tools):\n",
    "    \"\"\"\n",
    "    * tools : 함수에 '@'데코레이터를 붙인 후 리스트에 담은 형태 필요\n",
    "    * action : 내부에 'Action' 키워드로 감싼 후, action, action_input이 포함된 dictionary를 'ReActJsonSingleInputOutputParser'클래스로 파싱처리한 데이터 input필요\n",
    "    \"\"\"\n",
    "    tool_info = tool_arg_generator(user_input=user_input, tools=tools)\n",
    "    action = tool_executor_parser(tool_info)\n",
    "    \n",
    "    from langgraph.prebuilt import ToolExecutor\n",
    "    tool_executor = ToolExecutor(tools)\n",
    "    print('\\n\\n\\n======================== TOOL RETURN ========================')\n",
    "    return tool_executor.invoke(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf11d0a7-1d8d-4e05-8ab5-38fcf2ce4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> User Question : 데이터사이언스셀의 팀원정보 보여줘\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2465.52 ms\n",
      "llama_print_timings:      sample time =      55.90 ms /    38 runs   (    1.47 ms per token,   679.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4512.68 ms /   924 tokens (    4.88 ms per token,   204.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1793.84 ms /    37 runs   (   48.48 ms per token,    20.63 tokens per second)\n",
      "llama_print_timings:       total time =    6418.91 ms /   961 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* 모델을 통해 질문에 적합한 Tool 선택 및 Argument생성을 완료하였습니다.*************\n",
      "{  \n",
      "  \"action\": \"get-team-member\",  \n",
      "  \"action_input\": {  \n",
      "    \"Team Name\": \"데이터사이언스셀\"  \n",
      "  }  \n",
      "}\n",
      " ********************************************\n",
      "{  \n",
      "  \"action\": \"get-team-member\",  \n",
      "  \"action_input\": {  \n",
      "    \"Team Name\": \"데이터사이언스셀\"  \n",
      "  }  \n",
      "}\n",
      "\n",
      "\n",
      "1) Tool_Name : get_team_member \n",
      "2) Tool Input Argument : {'team_name': '데이터사이언스셀'}\n",
      "********************************************\n",
      "\n",
      "\n",
      "\n",
      "======================== TOOL RETURN ========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'손철준, 김민지D, 이재엽, 김원태, 이흥배, 허재우, 오형록, 신유나, 이승연C, 김상웅, 이창윤, 양성렬 총 12명입니다. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_input = \"서울의 날씨 알려줘\"\n",
    "# user_input = '판교 몽중헌의 전화번호 알려줘'\n",
    "# user_input = '오늘 날짜 알려줘'\n",
    "user_input = '데이터사이언스셀의 팀원정보 보여줘'\n",
    "# user_input = '판다는 보통 어디에 살지?'\n",
    "# user_input = '이흥배 주임연구원의 근무시간알려줘'\n",
    "cutom_Tool_Executor(user_input, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad854c-ea4d-421c-bbba-e2e06d5db686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
